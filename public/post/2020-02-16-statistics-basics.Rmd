---
title: Statistics Basics
author: Adrian Bandolon
date: '2020-02-16'
slug: statistics-basics
categories:
  - Data Science
  - Laboratory
  - R
  - Getting Started
tags:
  - R
  - Statistics
keywords:
  - R
  - Statistics
  - Getting Started
clearReading: true
thumbnailImage: /post/2020-02-16-statistics-basics_files/sundial.jpg
thumbnailImagePosition: left
autoThumbnailImage: no
---

Statistics is inherent in laboratory work since our primary product is data, yet so little emphasis is placed on it in medical technology schools. In this post we will cover basic statistical concepts that are widely used in the laboratory. We still first study probability distributions. In the process we will learn about measures of central tendency (i.e. mean, mode, median) and measures of dispersion (i.e. variance, standard deviation, coefficient of variation).

# Probability Distributions

A probability distribution is simply a description of the probability of a random phenomenon occuring. This description is often expressed as a mathematical function, called a [probability density function (PDF)](https://en.wikipedia.org/wiki/Probability_density_function). The PDF of a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) for example is:

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2} $$
where $\mu$ is the mean of the distribution and $\sigma$ is its standard deviation. If we are to plot this function with $\mu = 0$ and $\sigma = 4.68$, it would look something like this:

```{r setup, include=FALSE}
knitr::opts_chunk$set(package.startup.message = FALSE)
library(ggplot2)
library(knitr)
library(dplyr)
library(kableExtra)
library(reshape2)
```

```{r echo=FALSE, fig.width=2, fig.height=2, fig.align='center', error=FALSE, warning=FALSE, message=FALSE}
df <- rnorm(1000, mean = 0, sd = 5)

df2 <- rnorm(1000, mean = 0, sd = 15)


df <- as.data.frame(cbind(df, df2))

plottable <- as.data.frame(melt(df))

p1 <- ggplot(plottable, aes(x=value)) + geom_density(fill="#473C8B", alpha = 0.5) + 
    xlab("Value") + ylab("Density") + theme_linedraw()

p1
```

Probability distributions are generally classified as either:

1. **Discrete probability distribution** -- describes outcome probabilities for events that can be counted. Discrete data can be numeric (e.g. WBC count) or it can also be categorical (e.g. type of WBC--neutrophils, lymhocytes, monocytes, etc.). Examples of well-known discrete probability distributions are the *Poisson* and the *Bernoulli* distributions.

2. **Continuous probability distribution** -- describes outcome probabilities of events that results from infinitely many possible values from a range of values without gaps.  This data can only be numeric. Most results we get in the chemistry department, like glucose and sodium levels are considered continuous. The most common continuous probability distribution not just in the lab and out in the world is the *Normal* distribution also known as *Gaussian* distribution. In clinical chemistry we may encounter the *Log-Normal* distribution also.

Every distribution function has parameters (i.e. mean, standard deviation for the normal distribution). These parameters often describe the central tendency and the variation of the points within the distribution.

# Measures of Central Tendency

A measure of central tendency is a single value that attempts to summarize a distribution by identifying the central position within it. They represent the most common value in a distribution. 

The appropriate measure to use depends on the type of probability distribution you are working with. The mean or average is the most commonly encountered measure of central tendency in the lab. This is because we mostly work with the normally distributed data. However, there are instances where the mean is not an appropriate measure to use, even when the data is normal.

## Mean (Arithmetic)
More commonly called an *average*. This is $\mu$ in the equation above. The arithmetic mean is determined by adding all the data points then dividing by the number of points. I specify the *arithmetic mean* because there are other methods for calculating a mean. The [*geometric mean*](https://en.wikipedia.org/wiki/Geometric_mean) for example is the *n*th root of the product of the data. The geometric mean is used in the INR calculation in coagulation.

The arithmetic mean is sensitive to *outliers*. An outlier is a value that is unusual compared to the rest of the points. In the figure below the last (right most data point) is an outlier. The red line is the mean without accounting for the outlier. The blue line is the mean if the outlier is taken into account. Outliers can significantly influence the mean.

In cases where there are a significant number of outliers, a different measure would need to be utilized. An alternative to the mean is the median.

```{r, outlier}
# set seed to ensure "randomly" generated values
# are the same at each run
set.seed(123)

# generate ten random numbers with mean = 100 and sd = 10
seriesWithOutOutlier <- rnorm(10, mean = 100, sd = 10)

#mean without outlier
seriesWithOutOutlier_mean <- mean(seriesWithOutOutlier) 

# add an outlier to the series
seriesWithOutlier <- c(seriesWithOutOutlier,300)
```
```{r, outlierFig, echo=FALSE, fig.align='center', fig.width=6, fig.height=4}
# plot this data
plot(seriesWithOutlier, pch =21, bg = "grey50", cex= 1.5, ylab="")
abline(h=seriesWithOutOutlier_mean, col="red", lty = 2)
abline(h=mean(seriesWithOutlier), col = "blue", lty = 2)
```

## Median

If a series is sorted in order of magnitude, the median is the middle value. This works well if there are an  odd number of points. For an even number of points, the arithmetic mean of the two middle points will be the median.

The median is less affected by outliers and skewed data. In the figure below, the median lies directly over the mean (faint black line) of the series where the outlier was not included. The median was not affected by the outlier at all.

```{r, medianPlot, echo=FALSE, fig.align='center', fig.width=6, fig.height=4}
# plot this data
plot(seriesWithOutlier, pch =21, bg = "grey50", cex= 1.5, ylab="")
abline(h=median(seriesWithOutlier), col = "black", lty = 1)
abline(h=seriesWithOutOutlier_mean, col="red", lty = 2)
abline(h=mean(seriesWithOutlier), col = "blue", lty = 2)
```

## Mode 

The mode is the most frequent value in a series. Mode is most often used for categorical or discrete data. 

In the example below the mode is 50, because Neutrophils have the highest count.

```{r echo=FALSE}
cells <- c("Neutrophils", "Lymphocytes", "Monocytes", "Eosinophils", "Basophils")
counts <- c(50, 25, 9, 8, 8)

df <- as.data.frame(cbind(cells, counts))
colnames(df) <- c("Cell Type", "Count")

df %>% kable("html") %>% kable_styling(full_width = FALSE)
```

An issue with using mode is that it is not always unique. We could, for example have the same count for neutrophils and lymphocytes when performing a differential. 

```{r echo=FALSE}
counts <- c(37, 37, 9, 7, 10)

df <- as.data.frame(cbind(cells, counts))
colnames(df) <- c("Cell Type", "Count")

df %>% kable("html") %>% kable_styling(full_width = FALSE)
```

This is why mode is seldom used for continuous variables. Consider measuring the glucose level of 30 people. How likely is it for two or more people to have **exactly** the same glucose level (e.g. 94.5 mmol/L)? 

# Measures of Dispersion

Measures of dispersion attempt to describe how far away the points are from the center. Consider the figure below:

```{r echo=FALSE, fig.width=2, fig.height=2, fig.align='center', error=FALSE, warning=FALSE, message=FALSE}
p2 <- ggplot(plottable, aes(x=value, fill = variable)) + 
  geom_density(alpha = 0.5) + 
  scale_fill_manual(values = c("#473C8B", "#FF4040"))+
  xlab("Value") + ylab("Density") +
  theme_linedraw() +
  theme(legend.position = "none") 

p2
```

The mean for both distributions are the same ^[Approximately zero, but not exactly equal because we used a psuedorandom number generator], but the points of the taller peak is less "dispersed" than the other. So let's talk about how dispersion is measured below.

## Variance

Variance measures variability from the mean. It is a measure of how far each number in a set is from the mean, hence also from every other number in the set. The symbol for variance is $\sigma^2$. The formula for variance is:

$$\sigma^2 = \frac{\sum^{n}_{i=1}(x_i - x)^2}{n}$$

where $x_i=$ the $i^{th}$ data point, $x=$ the mean of all data points, $n=$ the number of data points.

A large variance indicates taht numbers in the set are from the mean and from each other. A small variance indicates the converse. A variance of zero indicates that all numbers in the set are identical. Variance can never be negative. 